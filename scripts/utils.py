import os
import sys
import base64
import json
import zipfile
import tempfile
from typing import List, Dict, Optional
import time

import requests

# Base Fabric REST API
FABRIC_API_BASE = "https://api.fabric.microsoft.com/v1"


class FabricAuthError(Exception):
    """Authentication/Token errors."""
    pass


class FabricApiError(Exception):
    """Fabric REST API call errors."""
    pass


def _get_env_or_fail(name: str) -> str:
    """Get an env var or raise a clear error."""
    value = os.getenv(name)
    if not value:
        raise FabricAuthError(f"Missing environment variable: {name}")
    return value


def get_access_token_spn() -> str:
    """
    R√©cup√®re un access token Microsoft Entra pour Fabric en client_credentials
    (Service Principal) vers le scope Fabric: https://api.fabric.microsoft.com/.default
    """
    tenant_id = _get_env_or_fail("FABRIC_TENANT_ID")
    client_id = _get_env_or_fail("FABRIC_CLIENT_ID")
    client_secret = _get_env_or_fail("FABRIC_CLIENT_SECRET")

    token_url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token"
    data = {
        "grant_type": "client_credentials",
        "client_id": client_id,
        "client_secret": client_secret,
        "scope": "https://api.fabric.microsoft.com/.default",
    }

    resp = requests.post(token_url, data=data)
    if resp.status_code != 200:
        raise FabricAuthError(
            f"Failed to acquire token. HTTP {resp.status_code}: {resp.text}"
        )

    token = resp.json().get("access_token")
    if not token:
        raise FabricAuthError("Token response does not contain 'access_token'.")
    return token


def fabric_request(method: str, path: str, token: str, **kwargs) -> requests.Response:
    """
    Appelle l'API Fabric REST (Core) :
      - Ajoute automatiquement le header Authorization: Bearer <token>
      - L√®ve une exception si le status HTTP n'est pas 2xx
    """
    url = f"{FABRIC_API_BASE}/{path.lstrip('/')}"
    headers = kwargs.pop("headers", {})
    headers["Authorization"] = f"Bearer {token}"

    if "json" in kwargs and "Content-Type" not in headers:
        headers["Content-Type"] = "application/json"

    print(f"Calling Fabric API: {method} {url}")
    resp = requests.request(method, url, headers=headers, **kwargs)

    if not resp.ok:
        raise FabricApiError(
            f"{method} {url} failed. "
            f"HTTP {resp.status_code}: {resp.text}"
        )

    return resp


def get_or_create_workspace(
    workspace_name: str,
    token: str,
    capacity_id: Optional[str] = None,
) -> str:
    """
    1. Liste les workspaces (GET /workspaces)
    2. Si un workspace avec displayName == workspace_name existe -> retourne son id
    3. Sinon, cr√©e le workspace (POST /workspaces)
    """
    resp = fabric_request("GET", "workspaces", token)
    data = resp.json()

    workspaces = data.get("value", data.get("workspaces", []))

    for ws in workspaces:
        if ws.get("displayName") == workspace_name:
            ws_id = ws.get("id")
            print(f"Workspace '{workspace_name}' already exists (id={ws_id}).")
            return ws_id

    body: Dict[str, object] = {"displayName": workspace_name}
    if capacity_id:
        body["capacityId"] = capacity_id

    print(f"Creating workspace '{workspace_name}'...")
    resp = fabric_request("POST", "workspaces", token, json=body)
    ws = resp.json()
    ws_id = ws["id"]
    print(f"Workspace created (id={ws_id}).")
    return ws_id


def list_items_by_type(
    workspace_id: str,
    item_type: str,
    token: str,
) -> List[Dict]:
    """
    Liste les items d'un workspace filtr√©s par type (Report, SemanticModel, ...)
      GET /workspaces/{workspaceId}/items?type={item_type}
    """
    path = f"workspaces/{workspace_id}/items?type={item_type}"
    resp = fabric_request("GET", path, token)
    data = resp.json()
    return data.get("value", data.get("items", []))


def build_definition_parts_from_folder(folder: str) -> List[Dict[str, str]]:
    """
    Construit la liste des 'parts' pour un Item Definition √† partir d'un dossier PBIP :
      - parcourt tous les fichiers (definition/, StaticResources/, .platform, etc.)
      - cr√©e un part par fichier:
          path       = chemin relatif (style 'definition/report.json')
          payload    = fichier encod√© en base64
          payloadType= InlineBase64 (unique valeur support√©e)
    """
    parts: List[Dict[str, str]] = []

    for root, _, files in os.walk(folder):
        for filename in files:
            full_path = os.path.join(root, filename)
            rel_path = os.path.relpath(full_path, folder).replace("\\", "/")

            with open(full_path, "rb") as f:
                content = f.read()

            b64 = base64.b64encode(content).decode("ascii")
            parts.append(
                {
                    "path": rel_path,
                    "payload": b64,
                    "payloadType": "InlineBase64",
                }
            )

    if not parts:
        raise ValueError(f"No files found in PBIP folder: {folder}")

    return parts


def wait_for_long_running_operation(
    operation_url: str,
    token: str,
    max_wait_seconds: int = 300,
    poll_interval: int = 5,
) -> Dict:
    """
    Suit une op√©ration longue dur√©e (Long Running Operation - LRO) via son URL.
    L'URL peut pointer vers l'API Fabric OU l'API Power BI (wabi-*).
    """
    print(f"\n‚è≥ Suivi de l'op√©ration: {operation_url}")
    
    start_time = time.time()
    attempt = 0
    
    while (time.time() - start_time) < max_wait_seconds:
        attempt += 1
        
        try:
            headers = {"Authorization": f"Bearer {token}"}
            resp = requests.get(operation_url, headers=headers)
            
            print(f"   [{attempt}] GET {operation_url} -> HTTP {resp.status_code}")
            
            if not resp.ok:
                print(f"   ‚ö†Ô∏è Erreur HTTP {resp.status_code}: {resp.text}")
                time.sleep(poll_interval)
                continue
            
            operation_status = resp.json()
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur lors du polling (tentative {attempt}): {e}")
            time.sleep(poll_interval)
            continue
        
        status = operation_status.get("status", "").lower()
        percent = operation_status.get("percentComplete", 0)
        
        if attempt == 1 or attempt % 10 == 0:
            print(f"   R√©ponse op√©ration: {json.dumps(operation_status, indent=2)}")
        
        print(f"   [{attempt}] Status: {status} ({percent}%)")
        
        if status in ["succeeded", "completed"]:
            print("   ‚úÖ Op√©ration termin√©e avec succ√®s")
            return operation_status
        
        elif status in ["failed", "cancelled"]:
            error_info = operation_status.get("error", operation_status)
            print(f"\n‚ùå √âCHEC DE L'OP√âRATION:")
            print(f"   Status: {status}")
            print(f"   Error: {json.dumps(error_info, indent=2)}")
            raise FabricApiError(
                f"Op√©ration {status}: {json.dumps(error_info)}"
            )
        
        elif status in ["running", "notstarted", "inprogress"]:
            time.sleep(poll_interval)
            continue
        
        else:
            print(f"   ‚ö†Ô∏è Statut inconnu: {status}, on continue...")
            time.sleep(poll_interval)
    
    raise FabricApiError(f"‚è±Ô∏è Timeout apr√®s {max_wait_seconds}s")


# ============================================================
# D√âPLOIEMENT DE RAPPORTS VIA API POWER BI
# ============================================================

def create_pbix_from_pbip(pbip_folder: str, output_path: Optional[str] = None) -> str:
    """
    Cr√©e un fichier .pbix temporaire depuis un dossier .Report PBIP.
    Le .pbix est un ZIP contenant tous les fichiers du PBIP.
    """
    if output_path is None:
        output_path = tempfile.mktemp(suffix=".pbix")
    
    print(f"üì¶ Cr√©ation du .pbix depuis {pbip_folder}...")
    
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(pbip_folder):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, pbip_folder)
                zipf.write(file_path, arcname)
    
    file_size = os.path.getsize(output_path) / (1024 * 1024)
    print(f"‚úÖ .pbix cr√©√©: {output_path} ({file_size:.2f} MB)")
    return output_path


def upload_pbix_via_powerbi_api(
    workspace_id: str,
    pbix_path: str,
    report_name: str,
    token: str,
) -> str:
    """
    Upload un fichier .pbix via l'API Power BI Import.
    
    Doc: https://learn.microsoft.com/en-us/rest/api/power-bi/imports/post-import-in-group
    """
    print(f"\nüì§ Upload du rapport via Power BI API...")
    print(f"   Fichier: {pbix_path}")
    print(f"   Nom: {report_name}")
    
    url = f"https://api.powerbi.com/v1.0/myorg/groups/{workspace_id}/imports"
    
    params = {
        "datasetDisplayName": report_name,
        "nameConflict": "CreateOrOverwrite",
    }
    
    headers = {
        "Authorization": f"Bearer {token}"
    }
    
    with open(pbix_path, 'rb') as f:
        files = {
            'file': (os.path.basename(pbix_path), f, 'application/octet-stream')
        }
        
        resp = requests.post(url, headers=headers, params=params, files=files)
    
    if not resp.ok:
        raise FabricApiError(
            f"Failed to upload .pbix: HTTP {resp.status_code}\n{resp.text}"
        )
    
    import_info = resp.json()
    import_id = import_info.get("id")
    
    print(f"‚úÖ Import d√©marr√© (id={import_id})")
    
    print(f"‚è≥ Attente de la fin de l'import...")
    return wait_for_import_completion(workspace_id, import_id, token)


def wait_for_import_completion(
    workspace_id: str,
    import_id: str,
    token: str,
    max_wait: int = 300
) -> str:
    """
    Attend la fin d'un import Power BI et retourne le report_id.
    """
    url = f"https://api.powerbi.com/v1.0/myorg/groups/{workspace_id}/imports/{import_id}"
    headers = {"Authorization": f"Bearer {token}"}
    
    start = time.time()
    while (time.time() - start) < max_wait:
        resp = requests.get(url, headers=headers)
        if not resp.ok:
            print(f"‚ö†Ô∏è Erreur v√©rification import: {resp.status_code}")
            time.sleep(5)
            continue
        
        import_status = resp.json()
        status = import_status.get("importState", "Unknown")
        
        print(f"   Status: {status}")
        
        if status == "Succeeded":
            reports = import_status.get("reports", [])
            if reports:
                report_id = reports[0]["id"]
                report_name = reports[0].get("name", "")
                print(f"‚úÖ Import termin√© - Report: {report_name} (id={report_id})")
                return report_id
            else:
                raise FabricApiError("Import succeeded but no report found")
        
        elif status == "Failed":
            error = import_status.get("error", "Unknown error")
            raise FabricApiError(f"Import failed: {error}")
        
        time.sleep(5)
    
    raise FabricApiError(f"Import timeout after {max_wait}s")


def rebind_report_to_dataset(
    workspace_id: str,
    report_id: str,
    dataset_id: str,
    token: str
) -> None:
    """
    Relie un rapport √† un dataset via l'API Power BI.
    Doc: https://learn.microsoft.com/en-us/rest/api/power-bi/reports/rebind-report-in-group
    """
    print(f"üîó Liaison du rapport au dataset...")
    print(f"   Report ID: {report_id}")
    print(f"   Dataset ID: {dataset_id}")
    
    url = f"https://api.powerbi.com/v1.0/myorg/groups/{workspace_id}/reports/{report_id}/Rebind"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }
    body = {
        "datasetId": dataset_id
    }
    
    resp = requests.post(url, headers=headers, json=body)
    
    if resp.ok:
        print(f"‚úÖ Rapport li√© au dataset avec succ√®s")
    else:
        print(f"‚ö†Ô∏è √âchec du rebind: HTTP {resp.status_code}")
        print(f"   {resp.text}")
        print(f"   Le rapport a √©t√© cr√©√© mais n'est pas li√© au dataset")


def deploy_report_via_fabric_workaround(
    workspace_id: str,
    pbip_folder: str,
    token: str,
) -> str:
    """
    D√©ploie un rapport en contournant le probl√®me de definition.pbir.
    Strat√©gie: Supprimer temporairement la r√©f√©rence au dataset, cr√©er le rapport vide,
    puis le relier au dataset via rebind.
    """
    report_name = os.path.basename(pbip_folder).replace(".Report", "")
    
    # Chercher le dataset associ√© depuis definition.pbir
    dataset_id = None
    dataset_name = None
    pbir_path = os.path.join(pbip_folder, "definition.pbir")
    
    try:
        if os.path.exists(pbir_path):
            with open(pbir_path, 'r', encoding='utf-8') as f:
                pbir_original = json.load(f)
                if "datasetReference" in pbir_original and "byPath" in pbir_original["datasetReference"]:
                    path = pbir_original["datasetReference"]["byPath"].get("path", "")
                    if path:
                        dataset_name = path.split("/")[-1].replace(".SemanticModel", "")
                        print(f"üîç Recherche du dataset '{dataset_name}'...")
                        items = list_items_by_type(workspace_id, "SemanticModel", token)
                        for item in items:
                            if item.get("displayName") == dataset_name:
                                dataset_id = item["id"]
                                print(f"‚úÖ Dataset trouv√©: {dataset_name} (id={dataset_id})")
                                break
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la recherche du dataset: {e}")
    
    if not dataset_id:
        print(f"‚ùå Impossible de d√©ployer le rapport sans dataset")
        print(f"   Assure-toi que le SemanticModel '{dataset_name}' existe dans le workspace")
        raise FabricApiError(f"Dataset '{dataset_name}' not found")
    
    # Construire les parts en modifiant temporairement definition.pbir
    print(f"üîß Modification temporaire de definition.pbir pour contourner les limitations...")
    parts = []
    
    for root, _, files in os.walk(pbip_folder):
        for filename in files:
            full_path = os.path.join(root, filename)
            rel_path = os.path.relpath(full_path, pbip_folder).replace("\\", "/")
            
            with open(full_path, "rb") as f:
                content = f.read()
            
            # Si c'est definition.pbir, on le modifie pour utiliser byConnection avec le dataset GUID
            if rel_path == "definition.pbir":
                pbir_modified = json.loads(content.decode('utf-8'))
                # Utiliser le GUID du dataset directement dans connectionString
                pbir_modified["datasetReference"] = {
                    "byConnection": {
                        "connectionString": f"semanticModelId={dataset_id}"
                    }
                }
                content = json.dumps(pbir_modified, indent=2).encode('utf-8')
                print(f"   ‚úèÔ∏è definition.pbir modifi√© avec dataset GUID: {dataset_id}")
            
            b64 = base64.b64encode(content).decode("ascii")
            parts.append({
                "path": rel_path,
                "payload": b64,
                "payloadType": "InlineBase64",
            })
    
    definition = {"parts": parts}
    
    # V√©rifier si le rapport existe d√©j√†
    print(f"\nüîç V√©rification si '{report_name}' existe d√©j√†...")
    existing_items = list_items_by_type(workspace_id, "Report", token)
    item_id = None
    for it in existing_items:
        if it.get("displayName") == report_name:
            item_id = it["id"]
            break
    
    # Cr√©er ou mettre √† jour
    if item_id is None:
        print(f"‚ûï Cr√©ation du rapport...")
        body = {
            "displayName": report_name,
            "type": "Report",
            "definition": definition,
        }
        
        resp = fabric_request("POST", f"workspaces/{workspace_id}/items", token, json=body)
        
        if resp.status_code == 202:
            location = resp.headers.get("Location")
            if location:
                retry_after = int(resp.headers.get("Retry-After", "5"))
                time.sleep(retry_after)
                wait_for_long_running_operation(location, token)
                
                # Trouver le rapport cr√©√©
                time.sleep(3)
                items = list_items_by_type(workspace_id, "Report", token)
                for it in items:
                    if it.get("displayName") == report_name:
                        item_id = it["id"]
                        break
        elif resp.status_code == 201:
            item = resp.json()
            item_id = item["id"]
    else:
        print(f"üîÑ Mise √† jour du rapport existant...")
        body = {"definition": definition}
        resp = fabric_request(
            "POST",
            f"workspaces/{workspace_id}/items/{item_id}/updateDefinition?updateMetadata=false",
            token,
            json=body
        )
        if resp.status_code == 202:
            location = resp.headers.get("Location")
            if location:
                wait_for_long_running_operation(location, token)
    
    if not item_id:
        raise FabricApiError("Failed to create or find report")
    
    print(f"‚úÖ Rapport cr√©√©/mis √† jour: {report_name} (id={item_id})")
    
    # Maintenant rebinder au bon dataset
    print(f"\nüîó Liaison au dataset {dataset_name}...")
    rebind_report_to_dataset(workspace_id, item_id, dataset_id, token)
    
    return item_id


# ============================================================
# FONCTION PRINCIPALE DE D√âPLOIEMENT
# ============================================================

def create_or_update_item_from_folder(
    workspace_id: str,
    folder: str,
    item_type: str,
    token: str,
) -> str:
    display_name = os.path.basename(folder)
    if "." in display_name:
        display_name = display_name.split(".", 1)[0]

    print(f"\n{'='*60}")
    print(f"üì¶ Publishing {item_type}: {display_name}")
    print(f"   Folder: {folder}")
    print(f"{'='*60}")

    # === TRAITEMENT SP√âCIAL POUR LES REPORTS ===
    if item_type == "Report":
        print("üéØ D√©ploiement rapport avec workaround Fabric")
        return deploy_report_via_fabric_workaround(workspace_id, folder, token)

    # === TRAITEMENT NORMAL POUR LES AUTRES ITEMS (SemanticModel, etc.) ===
    parts = build_definition_parts_from_folder(folder)
    print(f"   üìÑ {len(parts)} fichiers encod√©s")
    
    for part in parts[:5]:
        print(f"      - {part['path']}")
    if len(parts) > 5:
        print(f"      ... et {len(parts) - 5} autres fichiers")
    
    definition = {"parts": parts}

    print(f"\nüîç V√©rification si '{display_name}' existe d√©j√†...")
    existing_items = list_items_by_type(workspace_id, item_type, token)
    item_id = None
    for it in existing_items:
        if it.get("displayName") == display_name:
            item_id = it["id"]
            break

    # -------------------------
    # CASE 1 : CREATE
    # -------------------------
    if item_id is None:
        print(f"‚ûï Item n'existe pas, cr√©ation en cours...")
        
        body = {
            "displayName": display_name,
            "type": item_type,
            "definition": definition,
        }

        resp = fabric_request(
            "POST",
            f"workspaces/{workspace_id}/items",
            token,
            json=body,
        )

        status_code = resp.status_code
        print(f"\nüì° R√©ponse Fabric: HTTP {status_code}")

        if status_code == 201:
            try:
                item = resp.json()
                item_id = item["id"]
                print(f"‚úÖ Cr√©√© imm√©diatement (201) - id={item_id}")
                return item_id
            except Exception as e:
                print(f"‚ùå Erreur parsing JSON: {e}")
                raise FabricApiError("Failed to parse 201 response")
        
        elif status_code == 202:
            print("‚è≥ Cr√©ation asynchrone (202 Accepted)")
            
            location = resp.headers.get("Location")
            retry_after = resp.headers.get("Retry-After", "5")
            
            if location:
                print(f"   Location header trouv√©: {location}")
                print(f"   Retry-After: {retry_after}s")
                
                try:
                    time.sleep(int(retry_after))
                    wait_for_long_running_operation(location, token)
                    
                    print("\nüîç Recherche de l'item cr√©√©...")
                    time.sleep(3)
                    
                    items = list_items_by_type(workspace_id, item_type, token)
                    for it in items:
                        if it.get("displayName") == display_name:
                            item_id = it["id"]
                            print(f"‚úÖ Item trouv√© apr√®s op√©ration async - id={item_id}")
                            return item_id
                    
                    raise FabricApiError(
                        f"Op√©ration r√©ussie mais item '{display_name}' introuvable"
                    )
                    
                except Exception as e:
                    print(f"\n‚ùå Erreur lors du suivi de l'op√©ration: {e}")
                    raise
            
            else:
                print("‚ö†Ô∏è PAS DE LOCATION HEADER!")
                raise FabricApiError("202 response without Location header")
        
        else:
            print(f"‚ùå Code HTTP inattendu: {status_code}")
            print(resp.text)
            raise FabricApiError(f"Unexpected status code {status_code}")

    # -------------------------
    # CASE 2 : UPDATE
    # -------------------------
    print(f"üîÑ Item existe d√©j√† (id={item_id}), mise √† jour...")
    
    body = {"definition": definition}

    resp = fabric_request(
        "POST",
        f"workspaces/{workspace_id}/items/{item_id}/updateDefinition?updateMetadata=false",
        token,
        json=body,
    )

    status_code = resp.status_code
    print(f"üì° R√©ponse mise √† jour: HTTP {status_code}")

    if status_code == 200:
        print(f"‚úÖ Mis √† jour imm√©diatement (200)")
        return item_id
    
    elif status_code == 202:
        location = resp.headers.get("Location")
        if location:
            wait_for_long_running_operation(location, token)
        else:
            print("‚ö†Ô∏è Pas de Location, attente arbitraire de 10s...")
            time.sleep(10)
        
        print(f"‚úÖ Mis √† jour (async)")
        return item_id
    
    else:
        print(f"‚ùå Mise √† jour √©chou√©e: {status_code}")
        print(resp.text)
        raise FabricApiError(f"Update failed with status {status_code}")